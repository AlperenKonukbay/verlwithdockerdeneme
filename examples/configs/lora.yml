model:
  name_or_path: "facebook/opt-125m"  # Use a small model for testing
  dtype: "float16"

lora:
  use_lora: true
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

train:
  output_dir: "./lora_output"
  batch_size: 4
  gradient_accumulation_steps: 1
  lr: 5e-5
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01
  max_grad_norm: 1.0
  epochs: 3

algorithm:
  type: "grpo"
  kl_coef: 0.1